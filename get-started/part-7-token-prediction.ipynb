{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## post prediction token selction\n",
    "\n",
    "### Greedy Search\n",
    "\n",
    "Pick top possible token\n",
    "\n",
    "Pro - Simple\n",
    "Con - Once we got a bad token in middle of prediction, the entire sentence could be doomed\n",
    "\n",
    "### Beam Search\n",
    "\n",
    "each time we keep **beam_num** of tokens, and process a BFS to create a exploration map.\n",
    "\n",
    "Pro - Keep trace for multi-steps\n",
    "Con - We could discard best solution over trim process, still have similar issue like greedy search\n",
    "\n",
    "### Top-K\n",
    "\n",
    "Pick from most possibility, until K tokens are picked. It called likehood scores\n",
    "\n",
    "```\n",
    "P(wi∣w1,...,wi−1)\n",
    "```\n",
    "\n",
    "Pro - optimize the change we got good tokens\n",
    "Con - when k == 1 it is pretty much Greedy, if K too large, we will need a LOT of compuation\n",
    "\n",
    "### Top-P\n",
    "\n",
    "When we got a list of possibility, we process a softmax on top of it, (this will garentee total possible will be 1). Then we pick a possilibity **P**, select from max possible choice, until sum of choosen elements have total possibility >= **p**.\n",
    "\n",
    "Pro - we will select more tokens, and based on possiblity, the selected word number will also changed\n",
    "Con - We need pick the **P** smartly, if the value too low, we got too many tokens, if too high, we got too few tokens\n",
    "\n",
    "### Tempurature\n",
    "\n",
    "The temperature parameter controls the smoothness of the output distribution. Lower values of temperature (close to 0) make the output more discrete (closer to a one-hot vector), while higher values make the distribution softer and more uniform. In the context of Gumbel-Softmax, adjusting the temperature allows for a trade-off between exploration and exploitation during training.\n",
    "\n",
    "$$\n",
    "P_i = \\frac{e^{\\frac{z_i}{T}}}{\\sum_j e^{\\frac{z_j}{T}}}\n",
    "$$\n",
    "\n",
    "z_i = logit for ith word\n",
    "T = temperature hyper parameter\n",
    "Pi = final possibility\n",
    "\n",
    "if T < 1, selection will be more focused\n",
    "T > 1 selection will be more random\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
